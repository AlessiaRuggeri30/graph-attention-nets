{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MoNet.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"7HPxbz9IZ1xO","colab_type":"code","colab":{}},"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kIH9phOegalY","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install tensorflow-gpu==2.0.0-alpha0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5PIfldx7gczn","colab_type":"code","colab":{}},"cell_type":"code","source":["import os,sys,inspect\n","import os\n","import joblib\n","import tensorflow as tf\n","import numpy as np\n","import h5py\n","import scipy.sparse.linalg as la\n","import scipy.sparse as sp\n","import scipy\n","import time\n","import pickle\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Layer, Dense, Flatten, Activation, Dropout, LeakyReLU\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import Model\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","%matplotlib inline\n","\n","import scipy.io as sio"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m0SNsOTAotwp","colab_type":"code","colab":{}},"cell_type":"code","source":["%aimport graph\n","%aimport coarsening\n","%aimport utils"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EXFo5cxHpBqH","colab_type":"code","colab":{}},"cell_type":"code","source":["# Graphs.\n","number_edges = 8\n","metric ='euclidean'\n","normalized_laplacian = True\n","coarsening_levels = 4\n","len_img = 28\n","\n","np.random.seed(0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mXMSAHpUplhM","colab_type":"code","colab":{}},"cell_type":"code","source":["# Useful functions\n","\n","def grid_graph(m):\n","  z = graph.grid(m)  # normalized nodes coordinates\n","  dist, idx = graph.distance_sklearn_metrics(z, k=number_edges, metric=metric) \n","  #dist contains the distance of the 8 nearest neighbors for each node indicated in z sorted in ascending order\n","  #idx contains the indexes of the 8 nearest for each node sorted in ascending order by distance\n","\n","  A = graph.adjacency(dist, idx)  # graph.adjacency() builds a sparse matrix out of the identified edges computing similarities as: A_{ij} = e^(-dist_{ij}^2/sigma^2)\n","\n","  return A, z\n","\n","  \n","def plot_matrix(m):\n","  plt.figure(figsize = (10, 10))\n","  plt.imshow(m.toarray())\n","  plt.show\n","\n","\n","# A friend helped me a lot with this function, so you will find something similar in other assignment\n","def coarsen_mnist(A, levels, nodes_coordinates):\n","    graphs, parents = coarsening.metis(A, levels) #Coarsen a graph multiple times using Graclus variation of the METIS algorithm. \n","                                                  #Basically, we randomly sort the nodes, we iterate on them and we decided to group each node\n","                                                  #with the neighbor having highest w_ij * 1/(\\sum_k w_ik) + w_ij * 1/(\\sum_k w_kj) \n","                                                  #i.e. highest sum of probabilities to randomly walk from i to j and from j to i.\n","                                                  #We thus favour strong connections (i.e. the ones with high weight wrt all the others for both nodes) \n","                                                  #in the choice of the neighbor of each node.\n","                    \n","                                                  #Construction is done a priori, so we have one graph for all the samples!\n","                    \n","                                                  #graphs = list of spare adjacency matrices (it contains in position \n","                                                  #          0 the original graph)\n","                                                  #parents = list of numpy arrays (every array in position i contains \n","                                                  #           the mapping from graph i to graph i+1, i.e. the idx of\n","                                                  #           node i in the coarsed graph -> that is, the idx of its cluster) \n","    perms = coarsening.compute_perm(parents) #Return a list of indices to reorder the adjacency and data matrices so\n","                                             #that two consecutive nodes correspond to neighbors that should be collapsed\n","                                             #to produce the coarsed version of the graph.\n","                                             #Fake nodes are appended for each node which is not grouped with anybody else\n","    \n","    coordinates = np.copy(nodes_coordinates)\n","    u_shape, u_rows, u_cols, u_val = [], [], [], []\n","    \n","    for i,A in enumerate(graphs):\n","        M, M = A.shape\n","\n","        # We remove self-connections created by metis.\n","        A = A.tocoo()\n","        A.setdiag(0)\n","\n","        if i < levels: #if we have to pool the graph \n","            A = coarsening.perm_adjacency(A, perms[i]) #matrix A is here extended with the fakes nodes\n","                                                       #in order to do an efficient pooling operation\n","                                                       #in tensorflow as it was a 1D pooling\n","\n","        A = A.tocsr()\n","        A.eliminate_zeros()\n","        \n","        Mnew, Mnew = A.shape\n","        u_shape.append(Mnew)\n","        \n","        if i == 0:\n","            fake_nodes = Mnew - M\n","            coordinates = np.concatenate([coordinates, np.ones([fake_nodes, 2])*np.inf], 0)\n","            coordinates = coordinates[perms[0]]\n","        \n","        start_node, end_node = A.nonzero()\n","        u_rows.append(start_node)\n","        u_cols.append(end_node)\n","        \n","        distance = coordinates[start_node] - coordinates[end_node]\n","        u_val.append(distance)\n","        \n","        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added), |E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz//2))\n","        \n","        # update coordinates for next coarser graph\n","        new_coordinates = []\n","        for k in range(A.shape[0]//2):\n","            idx_first_el = k * 2\n","            \n","            if not np.isfinite(coordinates[idx_first_el][0]):\n","                new_coordinates.append(coordinates[idx_first_el+1])\n","                \n","            elif not np.isfinite(coordinates[idx_first_el+1][0]):\n","                new_coordinates.append(coordinates[idx_first_el])\n","                \n","            else:\n","                new_coordinates.append(np.mean(coordinates[idx_first_el:idx_first_el+2], axis=0))\n","                \n","        coordinates = np.asarray(new_coordinates)\n","\n","    return u_shape, u_rows, u_cols, u_val, perms[0]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K0lm_TPFmTb2","colab_type":"code","outputId":"b1c11137-bc54-459d-d995-202559f80896","executionInfo":{"status":"ok","timestamp":1555585216150,"user_tz":-120,"elapsed":1085,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Load MNIST dataset\n","\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.astype(np.float32) / 255\n","x_test = x_test.astype(np.float32) / 255\n","y_train = y_train.astype(np.float32) \n","y_test = y_test.astype(np.float32)\n","\n","val_n = len(x_train)//100 * 15\n","(x_val, y_val) = x_train[0:val_n], y_train[0:val_n]\n","(x_train, y_train) = x_train[val_n:], y_train[val_n:]\n","x_val = x_val.astype(np.float32)\n","y_val = y_val.astype(np.float32)\n","\n","x_train_original = x_train.reshape(-1, len_img*len_img)\n","x_val_original = x_val.reshape(-1, len_img*len_img)\n","x_test_original = x_test.reshape(-1, len_img*len_img)\n","\n","print(x_train_original.shape)"],"execution_count":237,"outputs":[{"output_type":"stream","text":["(51000, 784)\n"],"name":"stdout"}]},{"metadata":{"id":"TSkRCyz2POPC","colab_type":"code","outputId":"d9440911-37a1-486b-bc1d-1aae99ab0e37","executionInfo":{"status":"ok","timestamp":1555585216151,"user_tz":-120,"elapsed":752,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["# Create u\n","\n","n_rows_cols = 28\n","A, nodes_coordinates = grid_graph(n_rows_cols)\n","u_shape, u_rows, u_cols, u_val, perm = coarsen_mnist(A, coarsening_levels, nodes_coordinates)\n","\n","u = []\n","for level in range(coarsening_levels):\n","    u.append([u_shape[level], u_rows[level], u_cols[level], u_val[level]])"],"execution_count":238,"outputs":[{"output_type":"stream","text":["Layer 0: M_0 = |V| = 992 nodes (208 added), |E| = 3198 edges\n","Layer 1: M_1 = |V| = 496 nodes (89 added), |E| = 1425 edges\n","Layer 2: M_2 = |V| = 248 nodes (37 added), |E| = 647 edges\n","Layer 3: M_3 = |V| = 124 nodes (13 added), |E| = 306 edges\n","Layer 4: M_4 = |V| = 62 nodes (0 added), |E| = 159 edges\n"],"name":"stdout"}]},{"metadata":{"id":"-ba6B2RolJkb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"79b53373-217b-4f66-c35e-5a67193518e5","executionInfo":{"status":"ok","timestamp":1555585218510,"user_tz":-120,"elapsed":2740,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}}},"cell_type":"code","source":["# Coarsen dataset\n","\n","x_train = coarsening.perm_data(x_train_original, perm)\n","x_val = coarsening.perm_data(x_val_original, perm)\n","x_test = coarsening.perm_data(x_test_original, perm)\n","\n","x_train = np.expand_dims(x_train, -1)\n","x_val = np.expand_dims(x_val, -1)\n","x_test = np.expand_dims(x_test, -1)\n","\n","print(x_train.shape)"],"execution_count":239,"outputs":[{"output_type":"stream","text":["(51000, 992, 1)\n"],"name":"stdout"}]},{"metadata":{"id":"9Zza-BK0mNOh","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create Gaussian weightning \n","\n","class Weighting(Layer):\n","\n","  def __init__(self, u, d, batch_size, **kwargs):\n","    self.u = u\n","    self.d = d\n","    self.batch_size = batch_size\n","    super(Weighting, self).__init__(**kwargs)\n","\n","  def build(self, input_shape):\n","    self.mu = self.add_weight(name='mu',\n","                              shape=(1, self.d),\n","                              initializer='uniform',\n","                              trainable=True)\n","    self.sigma = self.add_weight(name='sigma',\n","                                 shape=(1, self.d),\n","                                 initializer='uniform',\n","                                 trainable=True)\n","    super(Weighting, self).build(input_shape)  \n","\n","  def call(self, X):\n","    batch_size, n_nodes, n_features = X.shape\n","    batch_size = self.batch_size\n","    u_shape, u_rows, u_cols, u_val = self.u\n","\n","    diff =  tf.square(u_val - self.mu)\n","    factor = tf.square(self.sigma) + 1e-14\n","    weights = -0.5 * (diff / factor)\n","    weights = tf.math.reduce_sum(weights, axis=1)\n","\n","    weights = tf.SparseTensor(indices=np.vstack([u_rows, u_cols]).T, \n","                              values=weights, \n","                              dense_shape=[u_shape]*2)\n","    weights = tf.sparse.reorder(weights)\n","    weights = tf.sparse.softmax(weights)\n","    \n","    X_t = tf.reshape(tf.transpose(X, [1,2,0]), [n_nodes, batch_size * n_features])\n","    \n","    D = tf.sparse.sparse_dense_matmul(weights, X_t)\n","    D = tf.transpose(tf.reshape(D, [n_nodes, n_features, batch_size]), [2,0,1])\n","    \n","    return D\n","    \n","  def compute_output_shape(self, input_shape):\n","    return (input_shape[0], self.output_dim)\n","\n","  \n","# Create Gaussian (kernel) layer\n","\n","class MoNet(Layer):\n","\n","  def __init__(self, u, d, n_gaussian, n_hidden, batch_size, **kwargs):\n","    self.u = u\n","    self.d = d\n","    self.n_gaussian = n_gaussian\n","    self.n_hidden = n_hidden\n","    self.batch_size = batch_size\n","    \n","    self.weightings = []\n","    for k in range(self.n_gaussian):\n","      weighting = Weighting(self.u, self.d, self.batch_size, trainable=True)\n","      self.weightings.append(weighting)\n","      \n","    super(MoNet, self).__init__(**kwargs)\n","\n","  def build(self, input_shape):\n","    self.W = self.add_weight(name='W', \n","                             shape=(input_shape[-1] * self.n_gaussian, self.n_hidden),\n","                             initializer='uniform',\n","                             trainable=True)\n","\n","    super(MoNet, self).build(input_shape)  # Be sure to call this at the end\n","\n","  def call(self, X):\n","    weightings = []\n","    for k in range(self.n_gaussian):\n","      weighting = self.weightings[k](X)\n","      weightings.append(weighting)\n","    \n","    weights = tf.transpose(tf.stack(weightings), [1,2,3,0])\n","    \n","    batch_size, n_nodes, n_features = X.shape\n","    batch_size = self.batch_size\n","    weights = tf.reshape(weights, [batch_size*n_nodes, n_features*self.n_gaussian])\n","    \n","    h = weights @ self.W\n","    h = tf.reshape(h, [batch_size, n_nodes, self.n_hidden])\n","\n","    return h\n","\n","  def compute_output_shape(self, input_shape):\n","    return (input_shape[1], self.n_hidden)\n","\n","  \n","class MPool1(Layer):\n","      def __init__(self, p, **kwargs):\n","         self.p = p\n","         super(MPool1, self).__init__(**kwargs)\n","     \n","      def call(self, X):\n","        \"\"\"Max pooling of size p. Should be a power of 2 (this is possible thanks to the reordering we previously did).\"\"\"\n","        if self.p > 1:\n","          X = tf.expand_dims(X, 3)  # shape = N x M x F x 1\n","          X = tf.nn.max_pool(X, ksize=[1,self.p,1,1], strides=[1, self.p,1,1], padding='SAME')\n","          X = tf.squeeze(X, [3])  # shape = N x M/p x F\n","        return X\n","          \n","      def compute_output_shape(self, input_shape):\n","        return input_shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yTY1XkVFmO6j","colab_type":"code","colab":{}},"cell_type":"code","source":["# Hyperparameters\n","\n","epochs=10\n","# For this implementation, batch_size needs to be a divisor of the number of samples (x_train = 51000 samples)\n","batch_size = 50\n","d = 2\n","n_classes=10\n","n_gaussian = 25\n","\n","dropout = 0.5\n","learning_rate = 1e-4\n","reg = l2(5e-4)\n","momentum = 0.8\n","p = 4"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zye12JRhWRPc","colab_type":"code","colab":{}},"cell_type":"code","source":["# Build the model\n","\n","model = Sequential()\n","model.add(MoNet(u=u[0], d=d, n_gaussian=n_gaussian, n_hidden=32, batch_size=batch_size, trainable=True))\n","model.add(LeakyReLU())\n","model.add(MPool1(p=p))\n","model.add(MoNet(u=u[2], d=d, n_gaussian=n_gaussian, n_hidden=64, batch_size=batch_size, trainable=True))\n","model.add(LeakyReLU())\n","model.add(MPool1(p=p))\n","model.add(Flatten())\n","model.add(Dense(512, activation='relu', kernel_regularizer=reg))\n","model.add(Dropout(dropout))\n","model.add(Dense(n_classes, activation='softmax', kernel_regularizer=reg))\n","\n","model.compile(optimizer=tf.optimizers.RMSprop(learning_rate=learning_rate, momentum=momentum),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Mo0ke2hPVST0","colab_type":"code","outputId":"b2292518-4513-4f6e-8707-eaff263cab6f","executionInfo":{"status":"ok","timestamp":1555587795605,"user_tz":-120,"elapsed":1054233,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"cell_type":"code","source":["# Train the model\n","\n","model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size)"],"execution_count":252,"outputs":[{"output_type":"stream","text":["Train on 51000 samples, validate on 9000 samples\n","Epoch 1/10\n","51000/51000 [==============================] - 108s 2ms/sample - loss: 0.4449 - accuracy: 0.9330 - val_loss: 0.1792 - val_accuracy: 0.9792\n","Epoch 2/10\n","51000/51000 [==============================] - 101s 2ms/sample - loss: 0.1547 - accuracy: 0.9778 - val_loss: 0.1301 - val_accuracy: 0.9816\n","Epoch 3/10\n","51000/51000 [==============================] - 101s 2ms/sample - loss: 0.1210 - accuracy: 0.9801 - val_loss: 0.1068 - val_accuracy: 0.9841\n","Epoch 4/10\n","51000/51000 [==============================] - 103s 2ms/sample - loss: 0.1094 - accuracy: 0.9822 - val_loss: 0.1082 - val_accuracy: 0.9828\n","Epoch 5/10\n","51000/51000 [==============================] - 100s 2ms/sample - loss: 0.1047 - accuracy: 0.9827 - val_loss: 0.0982 - val_accuracy: 0.9850\n","Epoch 6/10\n","51000/51000 [==============================] - 100s 2ms/sample - loss: 0.0994 - accuracy: 0.9844 - val_loss: 0.1000 - val_accuracy: 0.9839\n","Epoch 7/10\n","51000/51000 [==============================] - 101s 2ms/sample - loss: 0.0948 - accuracy: 0.9853 - val_loss: 0.0969 - val_accuracy: 0.9850\n","Epoch 8/10\n","51000/51000 [==============================] - 106s 2ms/sample - loss: 0.0927 - accuracy: 0.9859 - val_loss: 0.0910 - val_accuracy: 0.9857\n","Epoch 9/10\n","51000/51000 [==============================] - 107s 2ms/sample - loss: 0.0924 - accuracy: 0.9854 - val_loss: 0.0903 - val_accuracy: 0.9858\n","Epoch 10/10\n","51000/51000 [==============================] - 110s 2ms/sample - loss: 0.0877 - accuracy: 0.9861 - val_loss: 0.1106 - val_accuracy: 0.9819\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fbe4498ecc0>"]},"metadata":{"tags":[]},"execution_count":252}]},{"metadata":{"id":"FVjDlOuwFb8x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"94cd8d6e-4456-4244-f2c4-97673c2ee91b","executionInfo":{"status":"ok","timestamp":1555587803935,"user_tz":-120,"elapsed":1061888,"user":{"displayName":"Alessia Ruggeri","photoUrl":"","userId":"01098494933204128714"}}},"cell_type":"code","source":["model.evaluate(x_test, y_test, batch_size=batch_size)"],"execution_count":253,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 9s 866us/sample - loss: 0.0883 - accuracy: 0.9865\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.08828231139108539, 0.9865]"]},"metadata":{"tags":[]},"execution_count":253}]},{"metadata":{"id":"PjM9d2Ktxpgx","colab_type":"code","colab":{}},"cell_type":"code","source":["# DEBUGGING\n","\n","# k = MoNet(u=u[0], d=d, n_gaussian=25, n_hidden=32, batch_size=batch_size, trainable=True)\n","# k(x_train[0:batch_size,:,:])"],"execution_count":0,"outputs":[]}]}